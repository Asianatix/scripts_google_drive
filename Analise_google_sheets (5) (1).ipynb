{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=887183825426-60s9l25rm89kp2ui22b1asrs58am4vgv.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&state=oQ59XLRjuhTDrnNFQqNUklalRp0NkA&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pickle\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from oauth2client import client\n",
    "from oauth2client import tools\n",
    "from oauth2client.file import Storage\n",
    "from apiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
    "import io\n",
    "from apiclient import errors\n",
    "from apiclient import http\n",
    "import logging\n",
    "from google.auth.transport.requests import Request\n",
    "from getfilelistpy import getfilelist\n",
    "import gspread\n",
    "import pandas as pd\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import os\n",
    "from pathlib import Path\n",
    "from apiclient import discovery\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib import rcParams\n",
    "\n",
    "os.chdir(\"/home/bruna_ramos/Downloads/data_science/annotation-routines/data/external/\")\n",
    "\n",
    "CLIENT_SECRET = \"client_secrets.json\"\n",
    "\n",
    "SCOPES=[\n",
    "    'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "if os.path.exists('token.pickle'):\n",
    "    with open('token.pickle', 'rb') as token:\n",
    "        creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET, SCOPES)  # credentials.json download from drive API\n",
    "        creds = flow.run_local_server()\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "\n",
    "#Com a pasta raíz informada identifica a pasta adicionada mais recentemente (batch atual) e traz as subpastas\n",
    "def folders_to_current_batch(FOLDER_ID):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts folders from current batch\n",
    "    \n",
    "    :param a: root folder\n",
    "    :return: folders from current batch\n",
    "    \"\"\"\n",
    "\n",
    "    folder_id=FOLDER_ID\n",
    "    \n",
    "# browse directories\n",
    "    results_folder_1 = service.files().list( q=\"mimeType='application/vnd.google-apps.folder' and parents in '\"+\\\n",
    "                                            folder_id+\"' and trashed = false\",fields=\\\n",
    "                                            \"nextPageToken, files(id, name)\",pageSize=400).execute()\n",
    "#specify subfolder names    \n",
    "    items_folder_1 = results_folder_1.get('files')\n",
    "    for items in items_folder_1:\n",
    "        if(items['name']=='sentenca'):\n",
    "            folder_2=items['id']       \n",
    "    results_folder_2 = service.files().list(q=\\\n",
    "                                            \"mimeType='application/vnd.google-apps.folder' and parents in '\"+\\\n",
    "                                            folder_2+\"' and trashed = false\",fields=\\\n",
    "                                            \"nextPageToken, files(id, name)\",pageSize=400).execute()\n",
    "    items_folder_2 = results_folder_2.get('files', [])    \n",
    "    for items in items_folder_2:\n",
    "        if(items['name']=='semantix_anotacoes_sentencas_201911'):\n",
    "            folder_3=items['id']\n",
    "            \n",
    "\n",
    "    #find current batch folder\n",
    "    \n",
    "    results_folder_3 = service.files().list(q=\"mimeType='application/vnd.google-apps.folder' and parents in '\"+\\\n",
    "                                            folder_3+\"' and trashed = false\",fields=\\\n",
    "                                            \"nextPageToken, files(id, name, createdTime)\",\\\n",
    "                                            orderBy=\"createdTime desc\").execute()\n",
    "    items_folder_3 = results_folder_3.get('files', [])    \n",
    "   \n",
    "\n",
    "    #change folder id in case of previous batch reprocessing\n",
    "    batch_atual=items_folder_3[0]['id']\n",
    "    name_batch_atual=items_folder_3[0]['name']\n",
    " \n",
    "    #batch_atual='1uUh2Pv8er4zw9bWcWfQtEV9WkYp_Womm'\n",
    "    #name_batch_atual='batch_002'\n",
    "    \n",
    "    results_batch = service.files().list(q=\"mimeType='application/vnd.google-apps.folder' and parents in '\"+\\\n",
    "                                         batch_atual+\"' and trashed = false\",fields=\\\n",
    "                                         \"nextPageToken, files(id, name)\",pageSize=400).execute()\n",
    "\n",
    "    dic = results_batch.get('files', [])\n",
    "\n",
    "    if not dic:\n",
    "        print('No files found.')\n",
    "    else:\n",
    "        #print('Files:')\n",
    "        total=[]\n",
    "        for i in range(len(dic)):\n",
    "            pastas=[]\n",
    "            for key,value in dic[i].items():\n",
    "                pastas.append((key,value,name_batch_atual))\n",
    "            total.append(pastas)\n",
    "    return total\n",
    "\n",
    "def check_folder_to_annotator(folder_id,diretorio,pastas):\n",
    "    \n",
    "    \"\"\"    \n",
    "    check if all annotators have folders    \n",
    "    :param a: folder id\n",
    "    :param b: folder name\n",
    "    :return: message of folders state\n",
    "    \"\"\"\n",
    "   \n",
    "    #authenticate access to google sheets\n",
    "    os.chdir(diretorio)\n",
    "    scope = ['https://spreadsheets.google.com/feeds']\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name('py-sheets-e691f630640c.json', scope)\n",
    "    gc = gspread.authorize(credentials)\n",
    "    \n",
    "    resource = {\n",
    "        \"oauth2\": creds,\n",
    "        \"fields\": \"files(name,id)\",\n",
    "        \"id\":folder_id,\n",
    "                }\n",
    "    res = getfilelist.GetFileList(resource) \n",
    "    dic = res.get('fileList')\n",
    "\n",
    "    if not dic:\n",
    "        print('No files found.')\n",
    "    else:\n",
    "        for i in dic:\n",
    "\n",
    "            for arquivo in i['files']:\n",
    "                if 'annotators_list' in arquivo['name']:\n",
    "                    lista_anotadores=arquivo\n",
    "\n",
    "    ident=lista_anotadores['id']\n",
    "    \n",
    "   #create dataframe with file data \n",
    "    wks = gc.open_by_key(ident).sheet1\n",
    "    data = wks.get_all_values()\n",
    "    headers = data.pop(0)\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    #check if all annotators have folders \n",
    "    lista_anotadores=[]\n",
    "    for anotador in df['anotadores']:\n",
    "        lista_anotadores.append(anotador)\n",
    "    \n",
    "   \n",
    "    lista_pastas=[]\n",
    "    for pasta in pastas:\n",
    "        lista_pastas.append((pasta[1][1]))\n",
    "\n",
    "    lista_final = list(set(lista_anotadores) - set(lista_pastas ))\n",
    "\n",
    "    if  not lista_final:\n",
    "        print(\"\\n Todos anotadores possuem pastas com seus nomes!\")\n",
    "    else:\n",
    "        print(\"\\n Nem todos anotadores possuem pastas, o diretório em falta é:\",lista_final)\n",
    "\n",
    "\n",
    "\n",
    "def find_files(folder_id,f_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts folders from current batch\n",
    "    \n",
    "    :param a: folder id\n",
    "    :param b: folder name\n",
    "    :return: list of 'arquivos' files\n",
    "    \"\"\"\n",
    "    folder_name = f_name\n",
    "    resource = {\n",
    "        \"oauth2\": creds,\n",
    "        \"fields\": \"files(name,id)\",\n",
    "        \"id\":folder_id,\n",
    "                }\n",
    "    res = getfilelist.GetFileList(resource)  \n",
    "\n",
    "    dic = res.get('fileList')\n",
    "    arquivos=[]\n",
    "    if not dic:\n",
    "        print('No files found.')\n",
    "    else:\n",
    "        for i in dic:\n",
    "    #select only the 'arquivos' spreadsheet\n",
    "            for arquivo in i['files']:\n",
    "                if 'arquivos' in arquivo['name']:\n",
    "                    arquivos.append(arquivo['name'])\n",
    "    if len(arquivos)>1:\n",
    "        print(\"\\n Lista de arquivos\",folder_name,arquivos)\n",
    "\n",
    "\n",
    "def check_folders_and_files(folder_id,folder_name,folder_txt,folder_pdf,folder_xml):\n",
    "         \n",
    "    \"\"\"    \n",
    "    check if folders have expected files    \n",
    "    :param a: folder id\n",
    "    :param b: folder name\n",
    "    :param c: txt folder name\n",
    "    :param d: pdf folder name\n",
    "    :param e: xml folder name\n",
    "    :return: inconsistent files and folders\n",
    "    \"\"\"\n",
    "\n",
    "    folder_name=folder_name\n",
    "\n",
    "    results=service.files().list(q=\"mimeType='application/vnd.google-apps.folder' and parents in '\"+folder_id+\"' and trashed = false\",fields=\"nextPageToken, files(id, name)\",pageSize=400).execute()\n",
    "\n",
    "    dic = results.get('files', [])\n",
    "    list_folders=[]\n",
    "\n",
    "    diretorio_ok=[]\n",
    "    diretorio_nok=[]\n",
    "    if not dic:\n",
    "        print('No folders found.')\n",
    "    else:\n",
    "        for i in dic:\n",
    "            list_folders.append(i['name'])\n",
    "    if (folder_txt in list_folders) and (folder_xml in list_folders)\\\n",
    "    and (folder_pdf in list_folders):\n",
    "         diretorio_ok.append((i['name'],i['id']))\n",
    "    if (folder_txt  not in list_folders):\n",
    "        print(\"\\n A pasta 'arquivos_texto' não está disponível para:\",folder_name)\n",
    "        \n",
    "    if ('arquivos_resultado' not in list_folders):\n",
    "        print(\"\\n A pasta 'arquivos_resultado' não está disponível para:\",folder_name)\n",
    "    if ('arquivos_originais' not in list_folders):\n",
    "        print(\"\\n A pasta 'arquivos_originais' não está criada para:\",folder_name)\n",
    "           \n",
    "\n",
    "\n",
    "    #checks if there are unexpected shapes and classifications folders\n",
    "    inconsistentes=[]\n",
    "    for i in dic:\n",
    "        f_name=i['name']\n",
    "        f_id=i['id']\n",
    "        \n",
    "        resource = {\"oauth2\": creds,\"fields\": \"files(name,id)\", \"id\":f_id,\"name\":f_name}\n",
    "        res = getfilelist.GetFileList(resource)\n",
    "\n",
    "        dic = res.get('fileList')\n",
    "        arquivos=[]\n",
    "        arquivos_inconsistentes=[]\n",
    "        \n",
    "        if not dic:\n",
    "            print('No files found.')\n",
    "        else:\n",
    "            for i in dic:\n",
    "                if not i['files']:\n",
    "                    print(\"\\n Diretório\",\"'\",resource['name'],\"'\",\"vazio para\",folder_name)\n",
    "                else:\n",
    "                    for x in i['files']:\n",
    "                    \n",
    "                        arquivos.append(x['name'])\n",
    "\n",
    "                    if resource['name']==folder_txt:\n",
    "                        for arquivo in arquivos:\n",
    "                            if(arquivo[-4::]!='.txt')&(arquivo[-4::]!='.TXT') | (\"(\" in arquivo):\n",
    "                                arquivos_inconsistentes.append((folder_name,resource['name'],arquivo))\n",
    "                    if resource['name']==folder_pdf:\n",
    "                        for arquivo in arquivos:\n",
    "                            if(arquivo[-4::] !='.pdf' )&(arquivo[-4::] !='.PDF' )| (\"(\" in arquivo):\n",
    "                                arquivos_inconsistentes.append((folder_name,resource['name'],arquivo))\n",
    "                    if f_name==folder_xml:\n",
    "                        for arquivo in arquivos:\n",
    "                            if(arquivo[-4::] !='.xml')&(arquivo[-4::] !='.XML')| (\"(\" in arquivo):\n",
    "                                arquivos_inconsistentes.append((resource['name'],arquivo,))\n",
    "                inconsistentes.append(arquivos_inconsistentes)\n",
    "\n",
    "    for i in inconsistentes:\n",
    "        if i:\n",
    "            print(\"\\n Há inconsistencia de arquivos no seguinte diretorio:\",i)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "#Cria diretórios locais para disponibilizar o arquivo excel de cada anotador\n",
    "def create_diretory(f_id,f_name,diretorio):\n",
    "    \n",
    "    \"\"\"    \n",
    "    create local directory    \n",
    "    :param a: folder id\n",
    "    :param b: folder name\n",
    "    :param c: local directory address\n",
    "    \"\"\"\n",
    "    \n",
    "    dire=diretorio+f_name\n",
    "    \n",
    "    if not os.path.exists(dire):\n",
    "        os.mkdir(dire)\n",
    "        \n",
    "def downloader(file_id,f_name,service):\n",
    "    \n",
    "    \"\"\"    \n",
    "    download files   \n",
    "    :param a: file id\n",
    "    :param b: file name\n",
    "    :param c: autentication serivice name\n",
    "    :return: progress download message\n",
    "    \"\"\"\n",
    "    \n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    fh = io.FileIO(f_name, 'wb')\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print('\\n Download do arquivo',file_id,f_name,(status.progress() * 100))\n",
    "        \n",
    "\n",
    "def download_files_xlsx(Folder_id,f_name,service,diretorio,nome_arquivo):\n",
    "    \n",
    "    \"\"\"    \n",
    "    download files   \n",
    "    :param a: folder id\n",
    "    :param b: folder name\n",
    "    :param c: local directory address\n",
    "    :return: progress download message\n",
    "    \"\"\"\n",
    "   \n",
    "    name_folder=f_name\n",
    "    folder_id=Folder_id\n",
    "    os.chdir(diretorio+name_folder)\n",
    "    \n",
    "    results = service.files().list(q=\"mimeType='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet' and parents in '\"+folder_id+\"' and trashed = false\",fields=\"nextPageToken, files(id, name)\").execute()\n",
    "    items = results.get('files', [])\n",
    "    for i in items:\n",
    "        if nome_arquivo in i['name']:\n",
    "            try:\n",
    "                downloader(i['id'],i['name'],service)\n",
    "            except IOError as e:\n",
    "                print (\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "\n",
    "\n",
    "def concat_files(pastas,diretorio,arquivo):\n",
    "    \n",
    "    \"\"\"    \n",
    "    download files   \n",
    "    :param a: list of folders\n",
    "    :param b: local directory address\n",
    "    :param c: downloaded file name\n",
    "    :return: concatened files as dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    appended_data=[]\n",
    "    for pasta in pastas:\n",
    "        os.chdir(diretorio+pasta[1][1])\n",
    "        if 'xlsx' in arquivo:\n",
    "            df=pd.read_excel(arquivo)\n",
    "            df.fillna('',inplace=True)\n",
    "            appended_data.append(df)\n",
    "\n",
    "    appended_data = pd.concat(appended_data)    \n",
    "    appended_data.reset_index(inplace=True)    \n",
    "    appended_data['batch_id']= int(pasta[0][2][-3::])    \n",
    "    appended_data['xml_file']=False \n",
    "    regex=\"[0-9]{7}[-][0-9]{2}[\\.][0-9]{4}[\\.][0-9]{1}[\\.][0-9]{2}[\\.][0-9]{4}\"\n",
    "    appended_data['cnj']=appended_data.to.apply(lambda x:re.findall(regex,x)[0])    \n",
    "    appended_data['batch_closing_date']=datetime.now()    \n",
    "    appended_data = appended_data[['annotator','batch_closing_date','batch_id','cnj','date','id','prediction'\\\n",
    "                                  ,'to','xml_file']]\n",
    "    appended_data.fillna(\"\",inplace=True)\n",
    "    appended_data['prediction'] =  appended_data['prediction'].str.strip()\n",
    "    \n",
    "    return appended_data\n",
    "\n",
    "def gera_lista_arquivos_ja_trabalhados(line):\n",
    "    \n",
    "    \"\"\"    \n",
    "    creat a list of sentence worked files ids   \n",
    "    :param a: dataframe line\n",
    "    :return: list of worked files ids\n",
    "    \"\"\"\n",
    "    \n",
    "    lista_base=['sentenca_homologatoria','sentenca_execucao','sentenca','ata_julgamento']\n",
    "    lista=[]\n",
    "    if line.prediction in lista_base:\n",
    "        lista=(line.id,line.annotator)\n",
    " \n",
    "    return lista\n",
    "\n",
    "\n",
    "def download_annotation_history(file_id,file_name,service,diretorio): \n",
    "    \n",
    "    \"\"\"    \n",
    "    download annotation histoty file   \n",
    "    :param a: file id\n",
    "    :param b: file name\n",
    "    :param c: service name\n",
    "    :param d: local directory address\n",
    "    :return: download progress\n",
    "    \"\"\"\n",
    "    \n",
    "    os.chdir(diretorio)\n",
    "    try:\n",
    "        downloader(file_id,file_name,service)\n",
    "    except IOError as e:\n",
    "        print (\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "        \n",
    "        \n",
    "        \n",
    "def list_files_xml(Folder_id,pasta_mae,pasta_xml,lista_arquivos_trabalhados):\n",
    "    \n",
    "    \"\"\"    \n",
    "    select xml files  \n",
    "    :param a: folder id\n",
    "    :param b: root folder\n",
    "    :param c: xml folder name\n",
    "    :param d: worked files list\n",
    "    :return: xml files\n",
    "    \"\"\"\n",
    "\n",
    "    folder_id=Folder_id    \n",
    "    folder_name=pasta_mae    \n",
    "    results=service.files().list(q=\"mimeType='application/vnd.google-apps.folder' and parents in '\"+folder_id+\"' and trashed = false\",fields=\"nextPageToken, files(id, name)\",pageSize=400).execute()\n",
    "    dic = results.get('files', [])\n",
    "    \n",
    "    for items in dic:\n",
    "        if items['name']==pasta_xml:\n",
    "            folder= items['id']\n",
    "\n",
    "    results = service.files().list(q=\"mimeType='text/xml' and parents in '\"+folder+\"' and trashed = false\",fields=\"nextPageToken, files(id, name)\").execute()\n",
    "    items = results.get('files', [])\n",
    "    list_files_xml=[int(i['name'][0:6]) for i in items]\n",
    "    l_trabalhado=[i[0] for i in lista_arquivos_trabalhados if i[1]== folder_name]\n",
    "    arquivos_e_xml=list(set(list_files_xml) - set(l_trabalhado))\n",
    "    \n",
    "    if len(arquivos_e_xml)>1:\n",
    "        print(\"\\n Há inconsistencia entre a anotação da planilha 'arquivo.xlsx' e o(s) documento(s) xml:\",arquivos_e_xml,\" da pasta:\", folder_name)\n",
    "\n",
    "    dicionario_xml = {}    \n",
    "    dicionario_xml[folder_name]=list_files_xml\n",
    "\n",
    "    return dicionario_xml\n",
    "\n",
    "def load_drive(diretorio,file_id, nome_do_arquivo, mimetype):\n",
    "    \n",
    "    \"\"\"    \n",
    "    load files in google drive  \n",
    "    :param a: local directory address\n",
    "    :param b: file id\n",
    "    :param c: file name\n",
    "    :param d: file mimetype\n",
    "    :return: upload status\n",
    "    \"\"\"\n",
    "\n",
    "    data_modificacao = lambda f: f.stat().st_mtime\n",
    "    directory = Path(diretorio)\n",
    "    files = directory.glob(nome_do_arquivo)\n",
    "    sorted_files = sorted(files, key=data_modificacao, reverse=True)\n",
    "    arquivo =sorted_files[0]\n",
    "    media_body = MediaFileUpload(\n",
    "            arquivo, \n",
    "            mimetype=mimetype)\n",
    " # Send the request to the API.\n",
    "    updated_file = service.files().update(\n",
    "                                fileId=file_id,\n",
    "                                media_body=media_body).execute()\n",
    "    print('o seguinte arquivo foi atualizado:',updated_file)\n",
    "    \n",
    "def alertas(row):\n",
    "    \n",
    "    \"\"\"    \n",
    "    create a alerts list  \n",
    "    :param a: dataframe line\n",
    "    :param b: file id\n",
    "    :param c: file name\n",
    "    :param d: file mimetype\n",
    "    :return: alerts list\n",
    "    \"\"\"\n",
    "   \n",
    "    alertas=[]\n",
    "    #list of possible ratings\n",
    "    lista_todos_arquivos=['acordao',\n",
    "                        'agravo_instrumento',\n",
    "                        'alvara_diversos',\n",
    "                        'alvara_levantamento',\n",
    "                        'ata_audiencia',\n",
    "                        'ata_julgamento',\n",
    "                        'contestacao',\n",
    "                        'contrarrazoes',\n",
    "                        'decisao_diversas',\n",
    "                        'decisao_embargos',\n",
    "                        'doc_diversos',\n",
    "                        'embargos_declaratorios',\n",
    "                        'embargos_execucao',\n",
    "                        'erro_no_arquivo',\n",
    "                        'guia_deposito',\n",
    "                        'guia_diversos',\n",
    "                        'peticao_inicial',\n",
    "                        'peticao_intermediaria',\n",
    "                        'peticao_juntada',\n",
    "                        'recurso_ordinario',\n",
    "                        'recurso_revista',\n",
    "                        'sentenca',\n",
    "                        'sentenca_homologatoria',\n",
    "                        'sentenca_execucao']\n",
    "    \n",
    "    #list of classifications that must have xml file\n",
    "    lista_xml=['sentenca_homologatoria','sentenca_execucao','sentenca','ata_julgamento']\n",
    "\n",
    "    if row.classificacao_arquivo not in lista_xml and row.xml_file=='xml disponivel':\n",
    "        alertas.append((\"inconsistencia na pasta de resultados de: \"+row.anotador+\", batch \"+(str(row.batch))\\\n",
    "                        +\". Não é esperado arquivo xml para o documento do tipo: \"\\\n",
    "                        +row.classificacao_arquivo+\",id do documento:\"+(str(row.id_documento))))\n",
    "    if (row.data == \"\"):\n",
    "        alertas.append((\" Há data não preenchida para\"+row.anotador+\", batch \"+(str(row.batch))\\\n",
    "                        +\"documento\"+(str(row.id_documento))))\n",
    "    if row.classificacao_arquivo not in lista_todos_arquivos:\n",
    "        alertas.append(\"Classificação inexistente de \"+row.anotador+\", batch \"+(str(row.batch))+\\\n",
    "                       \", id_documento:\"+(str(row.id_documento))+\". A classificação '\"+\\\n",
    "                       row.classificacao_arquivo+\"' é inválida\")\n",
    "        \n",
    "    return  alertas   \n",
    "\n",
    "def write_txt(lista_alertas,diretorio):\n",
    "    \n",
    "    \"\"\" \n",
    "    create a txt file with inconsistencies in files and folders\n",
    "    :param a: alerts list\n",
    "    :param b: local directory address\n",
    "    \"\"\"\n",
    "        \n",
    "    with open(diretorio+'log_anotacoes.txt', 'w', encoding='utf-8') as f:\n",
    "        for nome in lista_alertas:\n",
    "            f.write(nome + '\\n')    \n",
    "   \n",
    "    \n",
    "\n",
    "def update_report(diretorio,file_id):\n",
    "    \n",
    "    \"\"\" \n",
    "    create a txt file with inconsistencies in files and folders\n",
    "    :param a: local directory address\n",
    "    :param b: file_id\n",
    "    \"\"\"\n",
    "    \n",
    "    # First retrieve the file from the API.\n",
    "    file = service.files().get(fileId=file_id).execute()\n",
    "\n",
    "    # File's new content.\n",
    "    media_body = MediaFileUpload(\n",
    "            'controle_anotacoes.xlsx', \n",
    "            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')\n",
    "\n",
    "    # Send the request to the API.\n",
    "    updated_file = service.files().update(\n",
    "                                fileId=file_id,\n",
    "\n",
    "                                media_body=media_body).execute()\n",
    "    print('\\n O relatorio controle de anotações foi atualizado:',updated_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Todos anotadores possuem pastas com seus nomes!\n",
      "\n",
      " Download do arquivo 1ZMSpDPCOG8sXrGdX8ELv8r19lwDP2hVF arquivos.xlsx 100.0\n",
      "\n",
      " Download do arquivo 1ZKY6lG8zfBuVIFvhKmeaD5KezCQYW7dk arquivos.xlsx 100.0\n",
      "\n",
      " Download do arquivo 1ZXwG4lbZkMX7iDIqGZng72ThEblFFaac arquivos.xlsx 100.0\n",
      "\n",
      " Download do arquivo 1tOXtEiKYBIl0bCtJd__RpgqffMVfbumK arquivos.xlsx 100.0\n",
      "\n",
      " Download do arquivo 1Z-hAOqbq-P-kS-bmlZOUmv5_2qkZL_gB arquivos.xlsx 100.0\n",
      "\n",
      " Download do arquivo 1Qauxeq_HPXcodYCp95dbu2lfsC-AsD8w arquivos.xlsx 100.0\n",
      "\n",
      " Download do arquivo 1xLwAG8AOiMkHb073BE6x_FU_jX95OW29 arquivos.xlsx 100.0\n",
      "\n",
      " Download do arquivo 1U9qiDPA4LhHIzu8zvxOW1ERNIvUPC5iq arquivos.xlsx 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruna_ramos/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:361: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Download do arquivo 1SS8TK_FFstAvAUpDSaUiuZbpKunQyIzU annotation_history.csv 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruna_ramos/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o seguinte arquivo foi atualizado: {'kind': 'drive#file', 'id': '1SS8TK_FFstAvAUpDSaUiuZbpKunQyIzU', 'name': 'annotation_history.csv', 'mimeType': 'text/csv'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruna_ramos/anaconda3/lib/python3.6/site-packages/pandas/core/ops/__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o seguinte arquivo foi atualizado: {'kind': 'drive#file', 'id': '1h064RuVBVGPCBK93cZ3q1BVPm8rZ1QRB', 'name': 'controle_anotacoes.pdf', 'mimeType': 'application/pdf'}\n",
      "\n",
      " O relatorio controle de anotações foi atualizado: {'kind': 'drive#file', 'id': '1PXYNB3kmHvKs_FB6lUgMW7HRiBDd0y1N', 'name': 'controle_anotacoes.xlsx', 'mimeType': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruna_ramos/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:511: FutureWarning: Series.data is deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o seguinte arquivo foi atualizado: {'kind': 'drive#file', 'id': '1a90BmFj3bRqml5RagnB2dnjEHl_EEAaw', 'name': 'log_anotacoes.txt', 'mimeType': 'text/plain'}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    #root folder id\n",
    "    pastas = folders_to_current_batch('128GfOBbU_qW2OwHjJwadQfg9ZiHF0N9o')\n",
    "\n",
    "    #root folder id\n",
    "    check_folder_to_annotator('128GfOBbU_qW2OwHjJwadQfg9ZiHF0N9o',\"/home/bruna_ramos/Downloads/\",pastas)\n",
    "    \n",
    "    lista=[]\n",
    "    for i in range(len(pastas)):\n",
    "        lista.append(find_files(pastas[i][0][1],pastas[i][1][1]))\n",
    "        \n",
    "    lista_pastas_criadas=[]\n",
    "    for i in range(len(pastas)):\n",
    "        lista_pastas_criadas.append(check_folders_and_files(pastas[i][0][1],pastas[i][1][1],\\\n",
    "                                'arquivos_texto',\\\n",
    "                                'arquivos_originais',\\\n",
    "                                'arquivos_resultado'))\n",
    "    \n",
    "    # local directory address\n",
    "    for pasta in pastas:\n",
    "        create_diretory(pasta[0][1],pasta[1][1],\"/home/bruna_ramos/Downloads/anotadores/\")\n",
    "\n",
    "    for pasta in pastas:\n",
    "        download_files_xlsx(pasta[0][1],pasta[1][1],service,\"/home/bruna_ramos/Downloads/anotadores/\",\n",
    "                            'arquivo')\n",
    "\n",
    "    df=concat_files(pastas,\"/home/bruna_ramos/Downloads/anotadores/\",\"arquivos.xlsx\")\n",
    "        \n",
    "    lista=df.apply(gera_lista_arquivos_ja_trabalhados,axis=1)\n",
    "    lista_arquivos_trabalhados=[l for l in lista if l]\n",
    "\n",
    "\n",
    "    download_annotation_history('1SS8TK_FFstAvAUpDSaUiuZbpKunQyIzU','annotation_history.csv',\n",
    "                            service,\"/home/bruna_ramos/Downloads/annotation_history/\")\n",
    "\n",
    "\n",
    "\n",
    "    os.chdir(\"/home/bruna_ramos/Downloads/annotation_history\")\n",
    "\n",
    "    df_annotation=pd.read_csv('annotation_history.csv')\n",
    "\n",
    "    \n",
    "    lista_xml=[]\n",
    "    for pasta in pastas:\n",
    "        lista_xml.append((list_files_xml(pasta[0][1],pasta[1][1],'arquivos_resultado',\n",
    "                                         lista_arquivos_trabalhados)))\n",
    "        \n",
    "    # Defines in Dataframe which listed documents have xml files\n",
    "    for index,row in df.iterrows(): \n",
    "        for i in range(len(lista_xml)):\n",
    "            for key,value in lista_xml[i].items():\n",
    "                if row.annotator==key and row.id in value:\n",
    "                    df.at[index, 'xml_file'] = True\n",
    "\n",
    "    df_trabalhado=df.loc[(df['prediction'] != '')]\n",
    "\n",
    "    batch_atual=int(df_trabalhado.batch_id.unique())\n",
    "\n",
    "    for index,row in df_annotation.iterrows():\n",
    "        if row.batch_id == batch_atual:\n",
    "             df_annotation.drop(index, inplace=True)\n",
    "            \n",
    "    df_annotation=df_annotation.append(df_trabalhado)\n",
    "    \n",
    "    df_report=df_annotation\n",
    "    \n",
    "   \n",
    "    df_annotation= df_annotation[['annotator',\n",
    "                                'batch_closing_date',\n",
    "                                'batch_id','cnj',\n",
    "                                'date','id',\n",
    "                                'prediction','to','xml_file']]\n",
    "        \n",
    "    df_annotation.to_csv(\"/home/bruna_ramos/Downloads/annotation_history_upload/appended.csv\")\n",
    "    \n",
    "    df_report=df_annotation\n",
    "    \n",
    "    df_report.fillna(\"\",inplace=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    df_report.reset_index(drop=True, inplace=True)\n",
    "    df_report['date_formated']=df_report.date.map(lambda x: str(x)[0:10] if x != \"\" else x)\n",
    "   \n",
    "    \n",
    "    #load in google drive the anotation history file\n",
    "    load_drive(\"/home/bruna_ramos/Downloads/annotation_history_upload\",'1SS8TK_FFstAvAUpDSaUiuZbpKunQyIzU'\\\n",
    "               , '*.csv', 'text/csv')\n",
    "\n",
    "    # define used columns\n",
    "    df_report = df_report.rename(columns={'annotator':'anotador',\n",
    "                                              'batch_closing_date': 'data_captura_dados',\n",
    "                                             'batch_id':'batch',\n",
    "                                             'cnj':'cnj',\n",
    "                                             'id':'id_documento',\n",
    "                                             'prediction':'classificacao_arquivo',\n",
    "                                             'to':'diretorio_s3',\n",
    "                                              'xml_file':'xml_file',\n",
    "                                             'date_formated':'data'})\n",
    "\n",
    "    df_report=df_report[['anotador','data_captura_dados','batch','cnj','id_documento','classificacao_arquivo',\\\n",
    "                  'diretorio_s3','xml_file','data']]\n",
    "\n",
    "    df_report.loc[df_report[\"xml_file\"]== False,'xml_file'] = 'sem xml'\n",
    "    df_report.loc[df_report[\"xml_file\"]== True,'xml_file'] = 'xml disponivel'\n",
    "    df_report['classificacao_arquivo'] = df_report['classificacao_arquivo'].str.strip()\n",
    "    \n",
    "\n",
    "    rcParams.update({'figure.autolayout': True})\n",
    "    \n",
    "    # create a pdf with graphcs sequence\n",
    "    with PdfPages(\"/home/bruna_ramos/Downloads/Charts.pdf\") as export_pdf:\n",
    "\n",
    "        donne_per_batch= df_report.loc[df_report['classificacao_arquivo']!=''].groupby(['batch'])['batch']\\\n",
    ".agg(['count']).reset_index()\n",
    "        donne_per_batch = donne_per_batch.set_index(['batch'])\n",
    "        all_files_per_batch=df_report.groupby('batch')['batch'].count()\n",
    "        perc_done=(donne_per_batch['count']/all_files_per_batch)*100\n",
    "\n",
    "        perc_done.plot(kind='barh', stacked=True,title=\"Volume de documentos já classificados - por batch\\n\")\n",
    "\n",
    "        export_pdf.savefig()    \n",
    "        plt.close()\n",
    "    \n",
    "        donne_per_annotator = df.loc[(df['prediction']!='')].groupby(['annotator'])\\\n",
    "        ['id'].agg(['count']).reset_index()\n",
    "        do_per_annotator = df.loc[(df['id']!='')].groupby(['annotator'])\\\n",
    "        ['id'].agg(['count']).reset_index()\n",
    "        donne_per_annotator= donne_per_annotator.merge(do_per_annotator, left_on='annotator', right_on='annotator')\n",
    "        donne_per_annotator = donne_per_annotator.set_index(['annotator'])\n",
    "        donne_per_annotator['feito']= donne_per_annotator['count_x']/donne_per_annotator['count_y']\n",
    "        donne_per_annotator['meta'] = (donne_per_annotator['count_y'].sub(donne_per_annotator['count_x'], axis=0))/donne_per_annotator['count_y']\n",
    "\n",
    "        donne_per_annotator.drop('count_x', inplace=True, axis=1)\n",
    "        donne_per_annotator.drop('count_y', inplace=True, axis=1)\n",
    "        donne_per_annotator.plot(kind='bar', stacked=True,\\\n",
    "                                 title=\"Documentos por anotador - batch atual\\n\",rot=90)\n",
    "    \n",
    "        export_pdf.savefig()    \n",
    "        plt.close()\n",
    "    \n",
    "        donne_per_document = df.loc[(df['prediction']!='')].groupby(['prediction'])\\\n",
    "        ['id'].agg(['count']).sort_values(by ='count',ascending=False).reset_index()\n",
    "        donne_per_document = donne_per_document.set_index(['prediction'])\n",
    "\n",
    "        donne_per_document.plot(kind='bar', stacked=True,title=\"Classificação de documentos- batch atual\\n\",rot=90)\n",
    "\n",
    "        export_pdf.savefig()    \n",
    "        plt.close()\n",
    "        \n",
    "        all_donne_per_document = df_report.loc[(df_report['classificacao_arquivo']!='')].\\\n",
    "    groupby(['classificacao_arquivo'])['id_documento'].agg(['count']).sort_values(by ='count',\\\n",
    "                                                                              ascending=False).reset_index()\n",
    "        all_donne_per_document = all_donne_per_document.set_index(['classificacao_arquivo'])\n",
    "\n",
    "        all_donne_per_document.plot(kind='bar', stacked=True,title=\"CLassificação de documentos - todos os batches\\n\",rot=90)\n",
    "        \n",
    "        export_pdf.savefig()    \n",
    "        plt.close()        \n",
    "        \n",
    "        sentenca_batchs=df_report.loc[df_report['classificacao_arquivo'].str.contains('sentenca')]\\\n",
    "        .groupby(['batch'])['batch'].agg(['count']).reset_index()\n",
    "        sentenca_batchs = sentenca_batchs.set_index(['batch'])\n",
    "\n",
    "        sentenca_batchs.plot(kind='barh', stacked=True,title\\\n",
    "                             =\"Sentencas por batch\\n\",rot=0)\n",
    "        \n",
    "        export_pdf.savefig()    \n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "        all_files= df_report.loc[df_report['classificacao_arquivo']!=''].groupby(['batch'])['batch']\\\n",
    ".agg(['count']).reset_index()\n",
    "        all_files = all_files.set_index(['batch'])\n",
    "        only_sentence=df_report.loc[df_report['classificacao_arquivo'].str.contains('sentenca')].groupby(['batch'])['batch']\\\n",
    ".agg(['count']).reset_index()\n",
    "        only_sentence = only_sentence.set_index(['batch'])\n",
    "        donne_per_batch= all_files.merge(only_sentence, how=\"outer\",on =['batch'])\n",
    "\n",
    "        donne_per_batch = donne_per_batch.rename(columns={'count_x': 'total', 'count_y': 'sentenca'})\n",
    "        donne_per_batch['%_sentenca']=donne_per_batch['sentenca']/donne_per_batch['total']*100\n",
    "        donne_per_batch['%_nao_sentenca']= 100-donne_per_batch['%_sentenca']\n",
    "        donne_per_batch.drop('total', inplace=True, axis=1)\n",
    "        donne_per_batch.drop('sentenca', inplace=True, axis=1)\n",
    "\n",
    "        donne_per_batch.plot(kind='barh', stacked=True,\\\n",
    "                             title=\"% Documentos classificados\\n\",rot=0)\n",
    "        \n",
    "        export_pdf.savefig()    \n",
    "        plt.close()     \n",
    "        \n",
    "       \n",
    "    \n",
    "        doc_day= df_report.loc[(df_report['xml_file']=='xml disponivel') & \\\n",
    "                                 (df_report['classificacao_arquivo'].str.contains('sentenca'))].\\\n",
    "        groupby(['data'])['xml_file'].agg(['count']).sort_values(by='data').reset_index()\n",
    "        \n",
    "        ax=sns.catplot(x=\"data\",y='count',kind='point',data=doc_day)\n",
    "        plt.xlabel(\"data\")\n",
    "        plt.ylabel(\"contagem\")\n",
    "        plt.title(\"Sentenças com xml - todos batches\")\n",
    "        ax.fig.autofmt_xdate()\n",
    "        export_pdf.savefig()    \n",
    "        plt.close()\n",
    "        \n",
    "        docs_day_annotator= df_report.loc[(df_report['xml_file']=='xml disponivel') & \\\n",
    "                                 (df_report['classificacao_arquivo'].str.contains('sentenca'))].\\\n",
    "        groupby(['data','anotador'])['xml_file'].agg(['count']).sort_values(by='data').reset_index()\n",
    "        \n",
    "        datas=df_report['data'].unique()\n",
    "        anotadores=df_report['anotador'].unique()\n",
    "        index = pd.MultiIndex.from_product([datas, anotadores], names = [\"data\", \"anotador\"])\n",
    "\n",
    "        stage_table=pd.DataFrame(index = index).sort_values(by='data').reset_index()\n",
    "       \n",
    "        docs_day_annotator= docs_day_annotator.merge(stage_table, how=\"outer\",on =['data','anotador']).sort_values(by='data')\n",
    "        docs_day_annotator['count'].fillna(0, inplace=True)\n",
    "\n",
    "        g=sns.catplot(x=\"data\",y='count',hue='anotador',kind='point',data=docs_day_annotator)\n",
    "        plt.xlabel(\"data\")\n",
    "        plt.ylabel(\"contagem\")\n",
    "        plt.title(\"sentença com xml - todos batches\")\n",
    "        g._legend.remove()\n",
    "        plt.legend(bbox_to_anchor=(1.0,1.0), title='anotador')\n",
    "        g.fig.autofmt_xdate()\n",
    "        export_pdf.savefig()    \n",
    "        plt.close()\n",
    "        \n",
    "    #export charts   \n",
    "    \n",
    "    load_drive(\"/home/bruna_ramos/Downloads/\",'1h064RuVBVGPCBK93cZ3q1BVPm8rZ1QRB',\\\n",
    "               'Charts.pdf', 'application/pdf')\n",
    "\n",
    "    #tables to excel report\n",
    "    \n",
    "    analise = df_report.groupby(['batch','anotador','data','classificacao_arquivo', 'xml_file'])\\\n",
    "    ['xml_file'].agg(['count']).reset_index()\n",
    "    \n",
    "    per_day = df_report.groupby(['batch','data','anotador'])\\\n",
    "    ['data'].agg(['count']).reset_index()\n",
    "    \n",
    "    no_xml = df_report.loc[(df_report['xml_file']=='sem xml') & (df_report['classificacao_arquivo']\\\n",
    "                                                                 .str.contains('sentenca'))].\\\n",
    "    groupby(['batch','anotador','id_documento','classificacao_arquivo', 'xml_file'])\\\n",
    "    ['xml_file'].agg(['count']).reset_index()\n",
    "    \n",
    "\n",
    "    \n",
    "    os.chdir(\"/home/bruna_ramos/Downloads/annotation_report/\")\n",
    "\n",
    "    writer = pd.ExcelWriter('controle_anotacoes.xlsx', engine='xlsxwriter')\n",
    "# Write each dataframe to a different worksheet.\n",
    "    analise.to_excel(writer, sheet_name='anotacoes')\n",
    "    per_day.to_excel(writer, sheet_name='anotacoes_diarias')\n",
    "    no_xml.to_excel(writer, sheet_name='anotacoes_sem_xml')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "    writer.save()\n",
    "    \n",
    "    update_report(\"/home/bruna_ramos/Downloads/annotation_report/\",'1PXYNB3kmHvKs_FB6lUgMW7HRiBDd0y1N')\n",
    "    \n",
    "# Gerar alertas de inconsistencias\n",
    "\n",
    "    l_alertas=df_report.apply(alertas,axis=1)\n",
    "    lista_alertas=[alerta[0] for alerta in l_alertas if alerta]\n",
    "    \n",
    "    write_txt(lista_alertas,\"/home/bruna_ramos/Downloads/\")  \n",
    "    load_drive(\"/home/bruna_ramos/Downloads/\",'1a90BmFj3bRqml5RagnB2dnjEHl_EEAaw',\\\n",
    "               'log_anotacoes.txt', 'text/plain')\n",
    "    \n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
