{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=887183825426-60s9l25rm89kp2ui22b1asrs58am4vgv.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&state=XEK377mkgDh5hlVo9c8sPRoxHnPrQd&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "# Identifica as subpastas da pasta mãe\n",
    "\n",
    "from __future__ import print_function\n",
    "import pickle\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from oauth2client import client\n",
    "from oauth2client import tools\n",
    "from oauth2client.file import Storage\n",
    "from apiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
    "import io\n",
    "from apiclient import errors\n",
    "from apiclient import http\n",
    "import logging\n",
    "from google.auth.transport.requests import Request\n",
    "from getfilelistpy import getfilelist\n",
    "import gspread\n",
    "import pandas as pd\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import os\n",
    "\n",
    "from apiclient import discovery\n",
    "\n",
    "\n",
    "CLIENT_SECRET = \"/home/bruna_ramos/Downloads/client_secrets.json\"\n",
    "\n",
    "# If modifying these scopes, delete the file token.pickle.\n",
    "#SCOPES = ['https://www.googleapis.com/auth/drive','https://www.googleapis.com/auth/drive.metadata.readonly',\\\n",
    "          #'https://www.googleapis.com/auth/drive.readonly','https://www.googleapis.com/auth/drive.file']\n",
    "SCOPES=[\n",
    "    'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "if os.path.exists('token.pickle'):\n",
    "    with open('token.pickle', 'rb') as token:\n",
    "        creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET, SCOPES)  # credentials.json download from drive API\n",
    "        creds = flow.run_local_server()\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "service = build('drive', 'v3', credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Com a pasta raíz informada identifica a pasta adicionada mais recentemente (batch atual) e traz as subpastas\n",
    "def folders_to_current_batch(FOLDER_ID):\n",
    "\n",
    "#Folder_id = '1M4rfUSYNxbK4qE-of3e4hGcnm4h__ITP' # Enter The Downloadable folder ID From Shared Link\n",
    "    folder_id=FOLDER_ID\n",
    "\n",
    "#retorna a pasta do batch atual\n",
    "    results_folder_1 = service.files().list( q=\"mimeType='application/vnd.google-apps.folder' and parents in '\"+\\\n",
    "                                            folder_id+\"' and trashed = false\",fields=\\\n",
    "                                            \"nextPageToken, files(id, name)\",pageSize=400).execute()\n",
    "    items_folder_1 = results_folder_1.get('files')\n",
    "\n",
    "    for items in items_folder_1:\n",
    "        if(items['name']=='sentenca'):\n",
    "            folder_2=items['id']\n",
    "            \n",
    "\n",
    "    results_folder_2 = service.files().list(q=\\\n",
    "                                            \"mimeType='application/vnd.google-apps.folder' and parents in '\"+\\\n",
    "                                            folder_2+\"' and trashed = false\",fields=\\\n",
    "                                            \"nextPageToken, files(id, name)\",pageSize=400).execute()\n",
    "    items_folder_2 = results_folder_2.get('files', [])\n",
    "    \n",
    "    for items in items_folder_2:\n",
    "        folder_3=items['id']\n",
    "    \n",
    "    results_folder_3 = service.files().list(q=\"mimeType='application/vnd.google-apps.folder' and parents in '\"+\\\n",
    "                                            folder_3+\"' and trashed = false\",fields=\\\n",
    "                                            \"nextPageToken, files(id, name, createdTime)\",\\\n",
    "                                            orderBy=\"createdTime desc\").execute()\n",
    "    items_folder_3 = results_folder_3.get('files', [])    \n",
    "   \n",
    "\n",
    "    batch_atual=items_folder_3[0]['id']\n",
    "    name_batch_atual=items_folder_3[0]['name']\n",
    "\n",
    "    results_batch = service.files().list(q=\"mimeType='application/vnd.google-apps.folder' and parents in '\"+\\\n",
    "                                         batch_atual+\"' and trashed = false\",fields=\\\n",
    "                                         \"nextPageToken, files(id, name)\",pageSize=400).execute()\n",
    "\n",
    "    dic = results_batch.get('files', [])\n",
    "\n",
    "    if not dic:\n",
    "        print('No files found.')\n",
    "    else:\n",
    "        #print('Files:')\n",
    "        total=[]\n",
    "        for i in range(len(dic)):\n",
    "            pastas=[]\n",
    "            for key,value in dic[i].items():\n",
    "                pastas.append((key,value,name_batch_atual))\n",
    "            total.append(pastas)\n",
    "    return total\n",
    "\n",
    "\n",
    "pastas=folders_to_current_batch('17XL23HvyVeuvzKvPFPRWZNu_w6SUMUgX')\n",
    "\n",
    "def check_folder_to_annotator(folder_id,diretorio):\n",
    "\n",
    "    \n",
    "#Identifica os anotadores\n",
    "    scope = ['https://spreadsheets.google.com/feeds']\n",
    "\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(diretorio+'py-sheets-e691f630640c.json', scope)\n",
    "\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    #folder_id='17XL23HvyVeuvzKvPFPRWZNu_w6SUMUgX'\n",
    "\n",
    "    resource = {\n",
    "        \"oauth2\": creds,\n",
    "        \"fields\": \"files(name,id)\",\n",
    "        \"id\":folder_id,\n",
    "                }\n",
    "    res = getfilelist.GetFileList(resource) \n",
    "    dic = res.get('fileList')\n",
    "\n",
    "\n",
    "    if not dic:\n",
    "        print('No files found.')\n",
    "    else:\n",
    "        for i in dic:\n",
    "#Filtra para capturar apenas a planilha 'arquivos' que contém o registro dos arquivos trabalhados\n",
    "            for arquivo in i['files']:\n",
    "                if 'annotators_data' in arquivo['name']:\n",
    "                    lista_anotadores=arquivo\n",
    "\n",
    "\n",
    "    ident=lista_anotadores['id']\n",
    "    \n",
    "    wks = gc.open_by_key(ident).sheet1\n",
    "    data = wks.get_all_values()\n",
    "    headers = data.pop(0)\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "#Verfifica se todos os anotadores possuem pastas  \n",
    "    lista_anotadores=[]\n",
    "    for anotador in df['anotadores']:\n",
    "        lista_anotadores.append(anotador)\n",
    "    \n",
    "#pastas=identifica_pastas_do_batch_atual('1M4rfUSYNxbK4qE-of3e4hGcnm4h__ITP')\n",
    "    \n",
    "    lista_pastas=[]\n",
    "    for pasta in pastas:\n",
    "        lista_pastas.append((pasta[1][1]))\n",
    "\n",
    "    lista_final = list(set(lista_anotadores) - set(lista_pastas ))\n",
    "\n",
    "    if  not lista_final:\n",
    "        print(\"\\n Todos anotadores possuem pastas com seus nomes!\")\n",
    "    else:\n",
    "        print(\"\\n Nem todos anotadores possuem pastas, o diretório em falta é:\",lista_final)\n",
    "\n",
    "\n",
    "check_folder_to_annotator('17XL23HvyVeuvzKvPFPRWZNu_w6SUMUgX',\"/home/bruna_ramos/Downloads/\")\n",
    "\n",
    "def find_files(folder_id,f_name):\n",
    "    folder_name = f_name\n",
    "    resource = {\n",
    "        \"oauth2\": creds,\n",
    "        \"fields\": \"files(name,id)\",\n",
    "        \"id\":folder_id,\n",
    "                }\n",
    "    res = getfilelist.GetFileList(resource)  \n",
    "\n",
    "#Recebe todos os arquivos abaixo do diretório informado\n",
    "    dic = res.get('fileList')\n",
    "    arquivos=[]\n",
    "    if not dic:\n",
    "        print('No files found.')\n",
    "    else:\n",
    "        for i in dic:\n",
    "#Filtra para capturar apenas a planilha 'arquivos' que contém o registro dos arquivos trabalhados\n",
    "            for arquivo in i['files']:\n",
    "                if 'arquivos' in arquivo['name']:\n",
    "                    arquivos.append(arquivo['name'])\n",
    "    print(\"\\n Lista de arquivos\",folder_name,arquivos)\n",
    "\n",
    "\n",
    "#Gera uma lista com todos os arquivos identificados como arquivos de controle para cada anotador\n",
    "lista=[]\n",
    "for i in range(len(pastas)):\n",
    "    lista.append(find_files(pastas[i][0][1],pastas[i][1][1]))\n",
    "\n",
    "    \n",
    "# Verifica se há todas as  pastas (txt,xml e pdf)para cada anotador       \n",
    "\n",
    "def check_folders_and_files(folder_id,folder_name,folder_txt,folder_pdf,folder_xml):\n",
    "\n",
    "    folder_name=folder_name\n",
    "#busca todas as subpastas da pasta indicada\n",
    "\n",
    "    results=service.files().list(q=\"mimeType='application/vnd.google-apps.folder' and parents in '\"+folder_id+\"' and trashed = false\",fields=\"nextPageToken, files(id, name)\",pageSize=400).execute()\n",
    "\n",
    "    dic = results.get('files', [])\n",
    "\n",
    "\n",
    "    list_folders=[]\n",
    "\n",
    "    diretorio_ok=[]\n",
    "    diretorio_nok=[]\n",
    "    if not dic:\n",
    "        print('No folders found.')\n",
    "    else:\n",
    "        for i in dic:\n",
    "            list_folders.append(i['name'])\n",
    "            #list_folders_id_name.append((i['name'],i['id']))\n",
    "    if (folder_txt in list_folders) and (folder_xml in list_folders)\\\n",
    "    and (folder_pdf in list_folders):\n",
    "         diretorio_ok.append((i['name'],i['id']))\n",
    "    if (folder_txt  not in list_folders):\n",
    "        print(\"\\n A pasta 'arquivos_texto' não está disponível para:\",folder_name)\n",
    "        \n",
    "    if ('arquivos_resultado' not in list_folders):\n",
    "        print(\"\\n A pasta 'arquivos_resultado' não está disponível para:\",folder_name)\n",
    "    if ('arquivos_originais' not in list_folders):\n",
    "        print(\"\\n A pasta 'arquivos_originais' não está criada para:\",folder_name)\n",
    "           \n",
    "\n",
    "#percorre todas os arquivos das pastas com os arquivos para verficar se há algum arquivo fora do formato esperado            \n",
    "    inconsistentes=[]\n",
    "    for i in dic:\n",
    "        f_name=i['name']\n",
    "        f_id=i['id']\n",
    "        \n",
    "        resource = {\"oauth2\": creds,\"fields\": \"files(name,id)\", \"id\":f_id,\"name\":f_name}\n",
    "        res = getfilelist.GetFileList(resource)\n",
    "\n",
    "        dic = res.get('fileList')\n",
    "        arquivos=[]\n",
    "        arquivos_inconsistentes=[]\n",
    "        \n",
    "        if not dic:\n",
    "            print('No files found.')\n",
    "        else:\n",
    "            for i in dic:\n",
    "                if not i['files']:\n",
    "                    print(\"\\n Diretório\",\"'\",resource['name'],\"'\",\"vazio para\",folder_name)\n",
    "                else:\n",
    "                    for x in i['files']:\n",
    "                    \n",
    "                        arquivos.append(x['name'])\n",
    "\n",
    "                    if resource['name']==folder_txt:\n",
    "                        for arquivo in arquivos:\n",
    "                            if(arquivo[-4::]!='.txt') | (\"(\" in arquivo):\n",
    "                                arquivos_inconsistentes.append((folder_name,resource['name'],arquivo))\n",
    "                    if resource['name']==folder_pdf:\n",
    "                        for arquivo in arquivos:\n",
    "                            if(arquivo[-4::] !='.pdf' )| (\"(\" in arquivo):\n",
    "                                arquivos_inconsistentes.append((folder_name,resource['name'],arquivo))\n",
    "                    if f_name==folder_xml:\n",
    "                        for arquivo in arquivos:\n",
    "                            if(arquivo[-4::] !='.xml')| (\"(\" in arquivo):\n",
    "                                arquivos_inconsistentes.append((resource['name'],arquivo,))\n",
    "                inconsistentes.append(arquivos_inconsistentes)\n",
    "\n",
    "    if len(i['files'])!= 0 and len(inconsistentes)!= 0:\n",
    "            print(\"\\n Há inconsistencia de arquivos no seguinte diretorio:\",inconsistentes)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "lista_pastas_criadas=[]\n",
    "for i in range(len(pastas)):\n",
    "    lista_pastas_criadas.append(check_folders_and_files(pastas[i][0][1],pastas[i][1][1],\\\n",
    "                                'arquivos_texto',\\\n",
    "                                'arquivos_originais',\\\n",
    "                                'arquivos_resultado'))\n",
    "lista_pastas_criadas\n",
    "\n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria diretórios locais para disponibilizar o arquivo excel de cada anotador\n",
    "def create_diretory(f_id,f_name,diretorio):\n",
    "    \n",
    "    diretorio=diretorio\n",
    "    \n",
    "    if not os.path.exists(diretorio+f_name):\n",
    "        os.mkdir(diretorio+f_name)\n",
    "\n",
    "for pasta in pastas:\n",
    "    create_diretory(pasta[0][1],pasta[1][1],\"/home/bruna_ramos/Downloads/anotadores/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download de arquivo xlsx do drive\n",
    "def downloader(file_id,f_name,service):\n",
    "    \n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    fh = io.FileIO(f_name, 'wb')\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print((status.progress() * 100))\n",
    "        \n",
    "\n",
    "def download_files_xlsx(Folder_id,f_name,service,diretorio,nome_arquivo):\n",
    "    \n",
    "   \n",
    "    name_folder=f_name\n",
    "    folder_id=Folder_id\n",
    "    #os.chdir(diretorio+name_folder)\n",
    "    \n",
    "    #results = service.files().list(q=\"mimeType='application/vnd.google-apps.spreadsheet' and parents in '\"+folder_id+\"' and trashed = false\",fields=\"nextPageToken, files(id, name)\").execute()\n",
    "    results = service.files().list(q=\"mimeType='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet' and parents in '\"+folder_id+\"' and trashed = false\",fields=\"nextPageToken, files(id, name)\").execute()\n",
    "    items = results.get('files', [])\n",
    "    for i in items:\n",
    "        if nome_arquivo in i['name']:\n",
    "            try:\n",
    "                downloader(i['id'],diretorio+name_folder+'/'+i['name'],service)\n",
    "            except IOError as e:\n",
    "                print (\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "\n",
    "for pasta in pastas:\n",
    "    download_files_xlsx(pasta[0][1],pasta[1][1],service,\"/home/bruna_ramos/Downloads/anotadores/\",'arquivo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "def concat_files(pastas,diretorio,arquivo):\n",
    "\n",
    "    appended_data = []\n",
    "    for pasta in pastas:\n",
    "        #os.chdir(diretorio+pasta[1][1])\n",
    "        #if 'xlsx' in arquivo:\n",
    "        df=pd.read_excel(diretorio+pasta[1][1]+'/'+arquivo)\n",
    "        df.fillna('',inplace=True)\n",
    "        appended_data.append(df)\n",
    "\n",
    "    appended_data = pd.concat(appended_data)\n",
    "    appended_data.reset_index(inplace=True)\n",
    "    appended_data['batch_id']= int(pasta[0][2][-3::])\n",
    "    appended_data['xml_file']='False'\n",
    " \n",
    "    regex=\"[0-9]{7}[-][0-9]{2}[\\.][0-9]{4}[\\.][0-9]{1}[\\.][0-9]{2}[\\.][0-9]{4}\"\n",
    "    appended_data['cnj']=appended_data.to.apply(lambda x:re.findall(regex,x)[0])\n",
    "    \n",
    "    appended_data['batch_closing_date']=datetime.now()\n",
    "    \n",
    "    appended_data = appended_data[['annotator','batch_closing_date','batch_id','cnj','date','id','prediction'\\\n",
    "                                  ,'to','xml_file']]\n",
    "    appended_data.fillna(\"\",inplace=True)\n",
    "    return appended_data\n",
    "#appended_data.to_excel('appended.xlsx')\n",
    "\n",
    "\n",
    "df=concat_files(pastas,\"/home/bruna_ramos/Downloads/anotadores/\",\"arquivos.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_lista_arquivos_ja_trabalhados(line):\n",
    "    \n",
    "    lista_base=['sentenca_homologatoria','sentenca_execucao','sentenca','ata_julgamento']\n",
    "    lista=[]\n",
    "    if line.prediction in lista_base:\n",
    "        lista=line.id\n",
    " \n",
    "    return lista\n",
    "#\",\".join(flags)\n",
    "lista=df.apply(gera_lista_arquivos_ja_trabalhados,axis=1)\n",
    "\n",
    "lista_arquivos_trabalhados=[l for l in lista if l]\n",
    "lista_arquivos_trabalhados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_annotation_history(Folder_id,service,diretorio):\n",
    "    \n",
    "    folder_id=Folder_id\n",
    "    #os.chdir(diretorio)\n",
    "    if not os.path.exists(diretorio+'annotation_history'):\n",
    "        os.mkdir(diretorio+'annotation_history')\n",
    "\n",
    "    #navega pelos diretórios para chegar até diretório onde ha o arquivo com o histórico de anotações\n",
    "    results_folder_1 = service.files().list( q=\"mimeType='application/vnd.google-apps.folder' and parents in '\"+\\\n",
    "                                            folder_id+\"' and trashed = false\",fields=\\\n",
    "                                            \"nextPageToken, files(id, name)\",pageSize=400).execute()\n",
    "    items_folder_1 = results_folder_1.get('files')\n",
    "\n",
    "    for items in items_folder_1:\n",
    "        if(items['name']=='sentenca'):\n",
    "            folder_2=items['id']\n",
    "            \n",
    "    results = service.files().list(q=\"mimeType='text/csv' and parents in '\"+folder_2+\"' and trashed = false\",\\\n",
    "                                   fields=\"nextPageToken, files(id, name, createdTime)\",\\\n",
    "                                            orderBy=\"createdTime desc\").execute()\n",
    "    items = results.get('files', [])\n",
    "\n",
    "    try:\n",
    "        downloader(items[0]['id'],diretorio+'annotation_history/'+items[0]['name'],service)\n",
    "    except IOError as e:\n",
    "        print (\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_annotation_history('17XL23HvyVeuvzKvPFPRWZNu_w6SUMUgX',service,\"/home/bruna_ramos/Downloads/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incluir registros trabalhados na lista de arquivos atualizada\n",
    "\n",
    "#pega o ultimo arquivo baixado\n",
    "from pathlib import Path\n",
    "\n",
    "data_modificacao = lambda f: f.stat().st_mtime\n",
    "\n",
    "directory = Path(\"/home/bruna_ramos/Downloads/annotation_history\")\n",
    "files = directory.glob('*.csv')\n",
    "sorted_files = sorted(files, key=data_modificacao, reverse=True)\n",
    "arquivo =sorted_files[0]\n",
    "df_annotation=pd.read_csv(arquivo)\n",
    "#df_annotation = pd.read_csv(arquivo, parse_dates=['date'])\n",
    "df_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_xml(Folder_id,pasta_mae,pasta_xml):\n",
    "\n",
    "    folder_id=Folder_id\n",
    "    folder_name=pasta_mae\n",
    "    \n",
    "    results=service.files().list(q=\"mimeType='application/vnd.google-apps.folder' and parents in '\"+folder_id+\"' and trashed = false\",fields=\"nextPageToken, files(id, name)\",pageSize=400).execute()\n",
    "\n",
    "    dic = results.get('files', [])\n",
    "    \n",
    "    for items in dic:\n",
    "        if items['name']==pasta_xml:\n",
    "            folder= items['id']\n",
    "\n",
    "    results = service.files().list(q=\"mimeType='text/xml' and parents in '\"+folder+\"' and trashed = false\",fields=\"nextPageToken, files(id, name)\").execute()\n",
    "\n",
    "    items = results.get('files', [])\n",
    "    \n",
    "    list_files_xml=[int(i['name'][0:6]) for i in items]\n",
    "\n",
    "    arquivos_e_xml=list(set(list_files_xml) ^ set(lista_arquivos_trabalhados))\n",
    "\n",
    "    print(\"\\n Há inconsistencia entre a anotação da planilha 'arquivo.xlsx' e o(s) documento(s) xml:\",arquivos_e_xml,\" da pasta:\", folder_name)\n",
    "\n",
    "\n",
    "    dicionario_xml = {}\n",
    "\n",
    "    dicionario_xml[folder_name]=list_files_xml\n",
    "\n",
    "    return dicionario_xml\n",
    "\n",
    "lista_xml=[]\n",
    "for pasta in pastas:\n",
    "    lista_xml.append((list_files_xml(pasta[0][1],pasta[1][1],'arquivos_resultado')))\n",
    "lista_xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define no Dataframe quais documentos listados possuem arquivos xml\n",
    "\n",
    "import numpy as np\n",
    "for index,row in df.iterrows(): \n",
    "    for i in range(len(lista_xml)):\n",
    "        for key,value in lista_xml[i].items():\n",
    "            if row.annotator==key and row.id in value:\n",
    "                df.at[index, 'xml_file'] = \"True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera planilha de historicos de anotações\n",
    "\n",
    "df_trabalhado=df.loc[(df['prediction'] != '')]\n",
    "\n",
    "batch_atual=int(df.batch_id.unique())\n",
    "\n",
    "for index,row in df_annotation.iterrows():\n",
    "    if row.batch_id == batch_atual:\n",
    "         df_annotation.drop(index, inplace=True)\n",
    "            \n",
    "\n",
    "\n",
    "df_annotation=df_annotation.append(df_trabalhado)\n",
    "df_annotation.fillna(\"\",inplace=True)\n",
    "\n",
    "\n",
    "#df_annotation.reset_index(inplace=True)\n",
    "df_annotation.reset_index(drop=True, inplace=True)\n",
    "df_annotation['date_formated']=df_annotation.date.map(lambda x: str(x)[0:10] if x != \"\" else x)\n",
    "df_annotation\n",
    "\n",
    "#df_anotation.to_csv(\"/home/bruna_ramos/Downloads/annotation_history_upload/appended.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotation_history(diretorio,id_folder):\n",
    "\n",
    "    data_modificacao = lambda f: f.stat().st_mtime\n",
    "\n",
    "    directory = Path(diretorio)\n",
    "    files = directory.glob('*.csv')\n",
    "    sorted_files = sorted(files, key=data_modificacao, reverse=True)\n",
    "    arquivo =sorted_files[0]\n",
    "\n",
    "    now =datetime.now()\n",
    "    data_e_hora_em_texto = now.strftime('%d%m%Y_%H%M')\n",
    "    name='annotation_history_'+data_e_hora_em_texto+'.csv'\n",
    "    file_metadata = {'name': name,'parents':[id_folder]}\n",
    "    media = MediaFileUpload(arquivo,\n",
    "                        mimetype='text/csv')\n",
    "    file = service.files().create(body=file_metadata,\n",
    "                                    media_body=media,\n",
    "                                    fields= 'id').execute()\n",
    "    print ('Arquivo carregado com sucesso!\\nFile id:',file.get('id'))\n",
    "load_annotation_history(\"/home/bruna_ramos/Downloads/annotation_history_upload\",'1aJeV7gPUeazhQv5U3kiWFG4Qzopt5P0s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation= df_annotation[['annotator',\n",
    "                                'batch_closing_date',\n",
    "                                'batch_id','cnj',\n",
    "                                'date','id',\n",
    "                                'prediction','to','xml_file']]\n",
    "        \n",
    "    \n",
    "df_report=df_annotation\n",
    "\n",
    "df_report.reset_index(drop=True, inplace=True)\n",
    "df_report['date_formated']=df_report.date.map(lambda x: str(x)[0:10] if x != \"\" else x)\n",
    "   \n",
    "\n",
    "df_report = df_report.rename(columns={'annotator':'anotador',\n",
    "                                              'batch_closing_date': 'data_captura_dados',\n",
    "                                             'batch_id':'batch',\n",
    "                                             'cnj':'cnj',\n",
    "                                             'id':'id_documento',\n",
    "                                             'prediction':'classificacao_arquivo',\n",
    "                                             'to':'diretorio_s3',\n",
    "                                              'xml_file':'xml_file',\n",
    "                                             'date_formated':'data'})\n",
    "\n",
    "df_report=df_report[['anotador','data_captura_dados','batch','cnj','id_documento','classificacao_arquivo',\\\n",
    "                  'diretorio_s3','xml_file','data']]\n",
    "\n",
    "df_report.loc[df_report[\"xml_file\"]== False,'xml_file'] = 'sem xml'\n",
    "df_report.loc[df_report[\"xml_file\"]== True,'xml_file'] = 'xml disponivel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analise = df_annotation.groupby(['batch_id','annotator','date_formated','prediction', 'xml_file'])\\\n",
    "['xml_file'].agg(['count']).reset_index()\n",
    "analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_per_batch=df_report.loc[df_report['classificacao_arquivo']!=''].groupby(['batch'])['batch'].agg(['count']).reset_index()\n",
    "\n",
    "all_files_per_batch=df_report.loc[df_report['id_documento']].groupby(['batch'])['batch'].agg(['count']).reset_index()\n",
    "all_files_per_batch\n",
    "#1 329\n",
    "#3 842"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a quantidadde de anotações por dia(de todos os batchs)\n",
    "\n",
    "batch_atual=1\n",
    "doc_day_annotator= df_report.loc[(df_report['xml_file']=='xml disponivel') & \\\n",
    "                                 (df_report['classificacao_arquivo']=='sentenca')&(df_report['batch']==batch_atual)].\\\n",
    " groupby(['data','anotador'])\\\n",
    "['xml_file'].agg(['count']).sort_values(by='data').reset_index()\n",
    "doc_day_annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gera grafico de de volume percentual de sentenças e não sentenças de cada batch\n",
    "\n",
    "all_files= df_report.loc[df_report['classificacao_arquivo']!=''].groupby(['batch'])['batch']\\\n",
    ".agg(['count']).reset_index()\n",
    "all_files = all_files.set_index(['batch'])\n",
    "\n",
    "only_sentence=df_report.loc[df_report['classificacao_arquivo'].str.contains('sentenca')].groupby(['batch'])['batch']\\\n",
    ".agg(['count']).reset_index()\n",
    "only_sentence = only_sentence.set_index(['batch'])\n",
    "\n",
    "donne_per_batch= all_files.merge(only_sentence, how=\"outer\",on =['batch'])\n",
    "\n",
    "donne_per_batch = donne_per_batch.rename(columns={'count_x': 'total', 'count_y': 'sentenca'})\n",
    "donne_per_batch['%_sentenca']=donne_per_batch['sentenca']/donne_per_batch['total']*100\n",
    "donne_per_batch['%_nao_sentenca']= 100-donne_per_batch['%_sentenca']\n",
    "donne_per_batch.drop('total', inplace=True, axis=1)\n",
    "donne_per_batch.drop('sentenca', inplace=True, axis=1)\n",
    "\n",
    "\n",
    "donne_per_batch.plot(kind='barh', stacked=True,title=\"Analise do volume de documentos já classificados\\n\",rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donne_per_batch= df_report.loc[df_report['classificacao_arquivo']!=''].groupby(['batch'])['batch']\\\n",
    ".agg(['count']).reset_index()\n",
    "donne_per_batch = donne_per_batch.set_index(['batch'])\n",
    "all_files_per_batch=df_report.groupby('batch')['batch'].count()\n",
    "perc_done=(donne_per_batch['count']/all_files_per_batch)*100\n",
    "\n",
    "perc_done.plot(kind='barh', stacked=True,title=\"Volume de documentos já classificados por batch\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_donne_per_document = df_report.loc[(df_report['classificacao_arquivo']!='')].\\\n",
    "groupby(['classificacao_arquivo'])['id_documento'].agg(['count']).sort_values(by ='count',\\\n",
    "                                                                              ascending=False).reset_index()\n",
    "all_donne_per_document = all_donne_per_document.set_index(['classificacao_arquivo'])\n",
    "\n",
    "all_donne_per_document.plot(kind='bar', stacked=True,title=\"Volume de tipos de documentos (todos os batches)\\n\",rot=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "doc_day_annotator= df_report.loc[(df_report['xml_file']=='xml disponivel') & \\\n",
    "                                 (df_report['classificacao_arquivo'].str.contains('sentenca'))].\\\n",
    "groupby(['data','anotador'])['xml_file'].agg(['count']).sort_values(by='data').reset_index()\n",
    "datas= df_report.loc[df_report['data']!=''].groupby(['data','anotador'])['data'].agg(['count']).reset_index()\n",
    "#doc_day_annotator= doc_day_annotator.merge(datas, how=\"right\",on =['anotador']).sort_values(by='data')\n",
    "print(doc_day_annotator.head(5))\n",
    "#ax=sns.catplot(x=\"data\",y='count_x',kind='point',data=doc_day_annotator)\n",
    "#plt.xlabel(\"data\")\n",
    "#plt.ylabel(\"contagem\")\n",
    "#plt.title(\"Sentenças com xml - todos batches-full\")\n",
    "#ax.fig.autofmt_xdate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_day_annotator= df_report.loc[(df_report['xml_file']=='xml disponivel') & \\\n",
    "                                 (df_report['classificacao_arquivo'].str.contains('sentenca'))].\\\n",
    "groupby(['data','anotador'])['xml_file'].agg(['count']).sort_values(by='data').reset_index()\n",
    "        \n",
    "datas=df_report['data'].unique()\n",
    "anotadores=df_report['anotador'].unique()\n",
    "index = pd.MultiIndex.from_product([datas, anotadores], names = [\"data\", \"anotador\"])\n",
    "\n",
    "stage_table=pd.DataFrame(index = index).sort_values(by='data').reset_index()\n",
    "print(stage_table)\n",
    "print(docs_day_annotator)\n",
    "        \n",
    "docs_day_annotator= docs_day_annotator.merge(stage_table, how=\"outer\",on =['data','anotador']).sort_values(by='data')\n",
    "docs_day_annotator['count'].fillna(0, inplace=True)\n",
    "print(docs_day_annotator.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.id != '')].groupby(['batch_id'])['batch_id'].agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "\n",
    "with PdfPages(\"/home/bruna_ramos/Downloads/Charts.pdf\") as export_pdf:\n",
    "    done=pd.DataFrame()\n",
    "\n",
    "    per_trabalhado= int((df.loc[(df.prediction != '') &(df['batch']==batch_atual)].count()['prediction'])\\\n",
    "                        /(df.count()['id'])*100)\n",
    "\n",
    "    perc_nao_trabalhado=100-per_trabalhado\n",
    "\n",
    "    done.set_value(1, 'feito', per_trabalhado)\n",
    "    done.set_value(1, 'meta', perc_nao_trabalhado)\n",
    "    done.rename(index={1:\"documentos clasificados\"},inplace=True)\n",
    "\n",
    "\n",
    "    done.plot(kind='barh', stacked=True,title=\"Analise do volume de documentos já classificados\\n\",rot=0)\n",
    "\n",
    "    export_pdf.savefig()\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    donne_per_annotator = df.loc[(df['prediction']!='')&(df['batch']==batch_atual)].groupby(['annotator'])\\\n",
    "    ['id'].agg(['count']).reset_index()\n",
    "\n",
    "\n",
    "    do_per_annotator = df.loc[(df['id']!='')&(df['batch']==batch_atual)].groupby(['annotator'])\\\n",
    "    ['id'].agg(['count']).reset_index()\n",
    "\n",
    "    donne_per_annotator= donne_per_annotator.merge(do_per_annotator, left_on='annotator', right_on='annotator')\n",
    "    donne_per_annotator = donne_per_annotator.set_index(['annotator'])\n",
    "    donne_per_annotator['feito']= donne_per_annotator['count_x']/donne_per_annotator['count_y']\n",
    "    donne_per_annotator['meta'] = (donne_per_annotator['count_y'].sub(donne_per_annotator['count_x'], axis=0))/donne_per_annotator['count_y']\n",
    "\n",
    "    donne_per_annotator.drop('count_x', inplace=True, axis=1)\n",
    "    donne_per_annotator.drop('count_y', inplace=True, axis=1)\n",
    "\n",
    "    donne_per_annotator.plot(kind='bar', stacked=True,\\\n",
    "                                 title=\"Analise do volume de documentos já classificados\\n\",rot=90)\n",
    "    \n",
    "    export_pdf.savefig()    \n",
    "    plt.close()\n",
    "    \n",
    "    donne_per_document = df.loc[(df['prediction']!='')].groupby(['prediction'])\\\n",
    "    ['id'].agg(['count']).sort_values(by ='count',ascending=False).reset_index()\n",
    "    donne_per_document = donne_per_document.set_index(['prediction'])\n",
    "\n",
    "    donne_per_document.plot(kind='bar', stacked=True,title=\"Volume de tipos de documentos\\n\",rot=90)\n",
    "\n",
    "    export_pdf.savefig()    \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "        \n",
    "    g=sns.catplot(x=\"data\",y='count',hue='anotador',kind='point',data=doc_day_annotator)\n",
    "    g._legend.remove()\n",
    "        #g.set(xlim=(1, 15))\n",
    "    plt.legend(bbox_to_anchor=(1.0,1.0), title='anotador')\n",
    "    g.fig.autofmt_xdate()\n",
    "        #export_pdf.savefig()    \n",
    "        #plt.close()\n",
    "        \n",
    "    \n",
    "    doc_day= df_report.loc[(df_report['xml_file']=='xml disponivel') & \\\n",
    "                                 (df_report['classificacao_arquivo']=='sentenca')].\\\n",
    "    groupby(['data'])['xml_file'].agg(['count']).sort_values(by='data').reset_index()\n",
    "        \n",
    "    ax=sns.catplot(x=\"data\",y='count',kind='point',data=doc_day)\n",
    "    ax.fig.autofmt_xdate()\n",
    "    export_pdf.savefig()    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def alertas(row):\n",
    "    alertas=[]\n",
    "    lista_todos_arquivos=['acordao',\n",
    "                        'agravo_instrumento',\n",
    "                        'alvara_diversos',\n",
    "                        'alvara_levantamento',\n",
    "                        'ata_audiencia',\n",
    "                        'ata_julgamento',\n",
    "                        'contestacao',\n",
    "                        'contrarrazoes',\n",
    "                        'decisao_diversas',\n",
    "                        'decisao_embargos',\n",
    "                        'doc_diversos',\n",
    "                        'embargos_declaratorios',\n",
    "                        'embargos_execucao',\n",
    "                        'erro_no_arquivo',\n",
    "                        'guia_deposito',\n",
    "                        'guia_diversos',\n",
    "                        'peticao_inicial',\n",
    "                        'peticao_intermediaria',\n",
    "                        'peticao_juntada',\n",
    "                        'recurso_ordinario',\n",
    "                        'recurso_revista',\n",
    "                        'sentenca',\n",
    "                        'sentenca_homologatoria']\n",
    "    lista_xml=['sentenca']\n",
    "\n",
    "    if row.classificacao_arquivo not in lista_xml and row.xml_file=='xml disponivel':\n",
    "        alertas.append((\"inconsistencia na pasta de resultados de: \"+row.anotador\\\n",
    "                        +\". Não é esperao arquivo xml para o documento do tipo: \"\\\n",
    "                        +row.classificacao_arquivo+\",id do documento:\"+\", batch \"+(str(row.id_documento))))\n",
    "    if (row.data == \"\"):\n",
    "        alertas.append((\" Há data não preenchida para\"+row.anotador+\", batch \"+(str(row.batch))\\\n",
    "                        +\"documento\"+(str(row.id_documento))))\n",
    "    if row.classificacao_arquivo not in lista_todos_arquivos:\n",
    "        alertas.append(\"Classificação inexistente de \"+row.anotador+\", batch \"+(str(row.batch))+\\\n",
    "                       \", id_documento:\"+(str(row.id_documento))+\". A classificação '\"+ row.classificacao_arquivo+\"' é invalida\")\n",
    "        \n",
    "    return  alertas    \n",
    "alertas=df_report.apply(alertas,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista_alertas=[alerta[0] for alerta in alertas if alerta]\n",
    "print(lista_alertas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escrever_txt(lista_alertas,diretorio):\n",
    "    with open(diretorio+'log_anotacoes.txt', 'w', encoding='utf-8') as f:\n",
    "        for nome in lista_alertas:\n",
    "            f.write(nome + '\\n')\n",
    "\n",
    "def carregar_txt():\n",
    "    with open('meu_arquivo.txt', 'r', encoding='utf-8') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "\n",
    "escrever_txt(lista_alertas,\"/home/bruna_ramos/Downloads/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_drive(diretorio,file_id, nome_do_arquivo, mimetype):\n",
    "\n",
    "    data_modificacao = lambda f: f.stat().st_mtime\n",
    "\n",
    "    directory = Path(diretorio)\n",
    "    files = directory.glob(nome_do_arquivo)\n",
    "    sorted_files = sorted(files, key=data_modificacao, reverse=True)\n",
    "\n",
    "\n",
    "    arquivo =sorted_files[0]\n",
    "    \n",
    "    media_body = MediaFileUpload(\n",
    "            arquivo, \n",
    "            mimetype=mimetype)\n",
    "\n",
    "    # Send the request to the API.\n",
    "    updated_file = service.files().update(\n",
    "                                fileId=file_id,\n",
    "\n",
    "                                media_body=media_body).execute()\n",
    "    print('o seguinte arquivo foi atualizado:',updated_file)\n",
    "    \n",
    "\n",
    "load_drive(\"/home/bruna_ramos/Downloads/\",'1KRM-fodrZ6oHpAXKWTCEY1jTPFXZjUoM', 'log_anotacoes.txt', 'text/plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_day = df_annotation.groupby(['batch_id','date_formated','annotator'])\\\n",
    "['date_formated'].agg(['count']).reset_index()\n",
    "per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_xml = df_annotation.loc[(df_annotation['xml_file']=='False') & (df_annotation['prediction']=='sentenca')].groupby(['batch_id','annotator','prediction', 'xml_file'])\\\n",
    "['xml_file'].agg(['count']).reset_index()\n",
    "no_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_xml = df_annotation.loc[(df_annotation['xml_file']=='True') & (df_annotation['prediction']=='sentenca')].groupby(['date_formated','prediction', 'xml_file'])\\\n",
    "['xml_file'].agg(['count']).reset_index()\n",
    "com_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe de barras exibindo evolução de documentos classificados\n",
    "import matplotlib.pyplot as plt\n",
    "done=pd.DataFrame()\n",
    "\n",
    "per_trabalhado= int((df[df.prediction != ''].count()['prediction'])/(df.count()['id'])*100)\n",
    "\n",
    "perc_nao_trabalhado=100-per_trabalhado\n",
    "\n",
    "done.set_value(1, 'feito', per_trabalhado)\n",
    "done.set_value(1, 'meta', perc_nao_trabalhado)\n",
    "done.rename(index={1:\"documentos clasificados\"},inplace=True)\n",
    "\n",
    "\n",
    "done.plot(kind='barh', stacked=True,title=\"Analise do volume de documentos já classificados\\n\",rot=0)\n",
    "#plt.savefig(\"/home/bruna_ramos/Downloads/teste5.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "with PdfPages(\"/home/bruna_ramos/Downloads/Charts.pdf\") as export_pdf:\n",
    "    done=pd.DataFrame()\n",
    "\n",
    "    per_trabalhado= int((df[df.prediction != ''].count()['prediction'])/(df.count()['id'])*100)\n",
    "\n",
    "    perc_nao_trabalhado=100-per_trabalhado\n",
    "\n",
    "    done.set_value(1, 'feito', per_trabalhado)\n",
    "    done.set_value(1, 'meta', perc_nao_trabalhado)\n",
    "    done.rename(index={1:\"documentos clasificados\"},inplace=True)\n",
    "\n",
    "\n",
    "    done.plot(kind='barh', stacked=True,title=\"Analise do volume de documentos já classificados\\n\",rot=0)\n",
    "\n",
    "    export_pdf.savefig()\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    donne_per_annotator = df.loc[(df['prediction']!='')].groupby(['annotator'])\\\n",
    "    ['id'].agg(['count']).reset_index()\n",
    "\n",
    "\n",
    "    do_per_annotator = df.loc[(df['id']!='')].groupby(['annotator'])\\\n",
    "    ['id'].agg(['count']).reset_index()\n",
    "\n",
    "\n",
    "    donne_per_annotator= donne_per_annotator.merge(do_per_annotator, left_on='annotator', right_on='annotator')\n",
    "    donne_per_annotator = donne_per_annotator.set_index(['annotator'])\n",
    "    donne_per_annotator['feito']= donne_per_annotator['count_x']/donne_per_annotator['count_y']\n",
    "    donne_per_annotator['meta'] = (donne_per_annotator['count_y'].sub(donne_per_annotator['count_x'], axis=0))/donne_per_annotator['count_y']\n",
    "\n",
    "    donne_per_annotator.drop('count_x', inplace=True, axis=1)\n",
    "    donne_per_annotator.drop('count_y', inplace=True, axis=1)\n",
    "\n",
    "    donne_per_annotator.plot(kind='bar', stacked=True,title=\"Analise do volume de documentos já classificados\\n\",rot=0)\n",
    "    \n",
    "    export_pdf.savefig()\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    donne_per_document = df.loc[(df['prediction']!='')].groupby(['prediction'])\\\n",
    "    ['id'].agg(['count']).sort_values(by ='count',ascending=False).reset_index()\n",
    "    donne_per_document = donne_per_document.set_index(['prediction'])\n",
    "\n",
    "\n",
    "    donne_per_document.plot(kind='bar', stacked=True,title=\"Volume de tipos de documentos\\n\",rot=90)\n",
    "    export_pdf.savefig()\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    import seaborn as sns \n",
    "    sns.catplot(x=\"date_formated\",y='count',hue='annotator',kind='point',data=anotador_xml)\n",
    "    export_pdf.savefig()\n",
    "    \n",
    "    plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donne_per_annotator = df.loc[(df['prediction']!='')].groupby(['annotator'])\\\n",
    "['id'].agg(['count']).reset_index()\n",
    "donne_per_annotator\n",
    "\n",
    "do_per_annotator = df.loc[(df['id']!='')].groupby(['annotator'])\\\n",
    "['id'].agg(['count']).reset_index()\n",
    "do_per_annotator\n",
    "\n",
    "donne_per_annotator= donne_per_annotator.merge(do_per_annotator, left_on='annotator', right_on='annotator')\n",
    "donne_per_annotator = donne_per_annotator.set_index(['annotator'])\n",
    "donne_per_annotator['feito']= donne_per_annotator['count_x']/donne_per_annotator['count_y']\n",
    "donne_per_annotator['meta'] = (donne_per_annotator['count_y'].sub(donne_per_annotator['count_x'], axis=0))/donne_per_annotator['count_y']\n",
    "\n",
    "donne_per_annotator.drop('count_x', inplace=True, axis=1)\n",
    "donne_per_annotator.drop('count_y', inplace=True, axis=1)\n",
    "\n",
    "donne_per_annotator.plot(kind='bar', stacked=True,title=\"Analise do volume de documentos já classificados\\n\",rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anotações por tipo de documento\n",
    "\n",
    "donne_per_document = df.loc[(df['prediction']!='')].groupby(['prediction'])\\\n",
    "['id'].agg(['count']).sort_values(by ='count',ascending=False).reset_index()\n",
    "donne_per_document = donne_per_document.set_index(['prediction'])\n",
    "\n",
    "\n",
    "donne_per_document.plot(kind='bar', stacked=True,title=\"Volume de tipos de documentos\\n\",rot=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anotador_xml = df_annotation.loc[(df_annotation['xml_file']=='True') & (df_annotation['prediction']=='sentenca')].groupby(['date_formated','annotator', 'xml_file'])\\\n",
    "['xml_file'].agg(['count']).reset_index()\n",
    "anotador_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(4.5, 4.5))\n",
    "sns.despine(f)\n",
    "sns_plot=sns.lineplot(x=\"date_formated\", y=\"count\",\n",
    "                hue=\"annotator\",\n",
    "                data=anotador_xml,ax=ax)\n",
    "#ax.set_xticklabels(labels, rotation=30)\n",
    "ticks =sns_plot.set_xticklabels(ax,rotation=45)\n",
    "#sns_plot.savefig('output.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "ax = sns.catplot(x=\"date_formated\",y='count',hue='annotator',kind='point',data=anotador_xml,legend=\"down\")\n",
    "ax.fig.autofmt_xdate()\n",
    "#sns.plt.legend(loc='upper left',bbox_to_anchor=(1,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from matplotlib import pyplot as plt\n",
    ">>> fig = plt.figure()\n",
    ">>> axis1 = fig.add_subplot(211)\n",
    ">>> axis1.plot(range(10))\n",
    ">>> axis2 = fig.add_subplot(212)\n",
    ">>> axis2.plot(range(10,20))\n",
    ">>> fig.savefig(\"multipleplots.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
